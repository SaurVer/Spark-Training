{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/08 21:26:51 WARN Utils: Your hostname, Saurabhs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.104 instead (on interface en0)\n",
      "24/08/08 21:26:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/08 21:26:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/08 21:27:06 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"spark application\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = spark.read.text('/Users/saurabhverma/Downloads/spark-3.5.1-bin-hadoop3/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['value']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, upper, col, cast\n",
    "\n",
    "split_strings = (strings\n",
    "                 .select(explode(split('value',' ')).alias('words'))\n",
    "                 .select(upper('words').alias('capital'))\n",
    "                 .filter(col('capital') != '')\n",
    "                 .groupBy('capital')\n",
    "                 .count()\n",
    "                 .orderBy('count', ascending=False)\n",
    "                         )\n",
    "\n",
    "# .select(explode(split(lower(col('value')), '\\s+')).alias('word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      capital|count|\n",
      "+-------------+-----+\n",
      "|          THE|   24|\n",
      "|           TO|   18|\n",
      "|          FOR|   16|\n",
      "|        SPARK|   15|\n",
      "|            A|   10|\n",
      "|          AND|   10|\n",
      "|           ##|    9|\n",
      "|           ON|    8|\n",
      "|          ```|    8|\n",
      "|          YOU|    7|\n",
      "|           IS|    7|\n",
      "|          RUN|    7|\n",
      "|      ```BASH|    6|\n",
      "|          CAN|    6|\n",
      "|         ALSO|    5|\n",
      "|           OF|    5|\n",
      "|           IN|    5|\n",
      "|           AN|    4|\n",
      "|DOCUMENTATION|    4|\n",
      "|    INCLUDING|    4|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_strings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transformations - orderby, groupby, filter, select, join\n",
    "# Actions - show, collect, save, take, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnm_data = spark.read.csv('/Users/saurabhverma/Spark-Training/Chapter2/mnm_dataset.csv', header= True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State| Color|Count|\n",
      "+-----+------+-----+\n",
      "|   TX|   Red|   20|\n",
      "|   NV|  Blue|   66|\n",
      "|   CO|  Blue|   79|\n",
      "|   OR|  Blue|   71|\n",
      "|   WA|Yellow|   93|\n",
      "|   WY|  Blue|   16|\n",
      "|   CA|Yellow|   53|\n",
      "|   WA| Green|   60|\n",
      "|   OR| Green|   71|\n",
      "|   TX| Green|   68|\n",
      "|   NV| Green|   59|\n",
      "|   AZ| Brown|   95|\n",
      "|   WA|Yellow|   20|\n",
      "|   AZ|  Blue|   75|\n",
      "|   OR| Brown|   72|\n",
      "|   NV|   Red|   98|\n",
      "|   WY|Orange|   45|\n",
      "|   CO|  Blue|   52|\n",
      "|   TX| Brown|   94|\n",
      "|   CO|   Red|   82|\n",
      "+-----+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnm_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import  sum\n",
    "\n",
    "grouped_data = (mnm_data\n",
    "                .groupBy(\"State\", 'Color')\n",
    "                .agg(sum('Count').alias('total'))\n",
    "                )\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State| Color|total|\n",
      "+-----+------+-----+\n",
      "|   WY| Green|94339|\n",
      "|   NV|   Red|89346|\n",
      "|   UT|  Blue|89977|\n",
      "|   WA|Orange|91521|\n",
      "|   NM| Green|91160|\n",
      "|   CA|  Blue|89123|\n",
      "|   WA|   Red|93332|\n",
      "|   NV| Brown|92478|\n",
      "|   AZ| Green|91882|\n",
      "|   CA|   Red|91527|\n",
      "|   AZ|Orange|91684|\n",
      "|   CO|  Blue|93412|\n",
      "|   NM|Orange|91251|\n",
      "|   NM|Yellow|92747|\n",
      "|   WY|Orange|87956|\n",
      "|   UT|Yellow|89264|\n",
      "|   WY|   Red|91768|\n",
      "|   OR|  Blue|90526|\n",
      "|   NV|Orange|93929|\n",
      "|   AZ|Yellow|90946|\n",
      "+-----+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
